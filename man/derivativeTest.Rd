% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/numeric.R
\name{derivativeTest}
\alias{derivativeTest}
\title{Testing derivatives based on the Taylor Theorem}
\usage{
derivativeTest(f, f_grad, h, W, D, ...)
}
\arguments{
\item{f}{scalar-valued function to be evaluated}

\item{f_grad}{function for derivative of \code{f}. Should take exact same arguments as \code{f}, and be a row vector.}

\item{h}{step sizes: must be (0,1]}

\item{W}{fixed constant to evaluate the function at. If multivariate, should be a column vector.}

\item{D}{direction to move in from W. If multivariate, should be a column vector}

\item{...}{Any other arguments to evaluate \code{f(x)} and \code{f_grad(x)}}
}
\value{
list of two vectors, \code{d1} holds the first order differences which have slope close to 1, \code{d2} should
be the second order differences which have slope close to 2.
}
\description{
Derivative check for gradients, Jacobians, and Hessians based on Taylor's Theorem.
}
\details{
In many optimizations, automatic differentiation and other numerical methods are preferred when
pursuing derivative-based optimization. However, derivatives are assumed to be wrong unless proven otherwise. This
derivative test makes use of properties of the Taylor expansion for any differentiable function to identify whether
a computed gradient is correct, given that the function is the correct one.
}
